{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMblQSYdqGt9YHJlL0cB0WX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "yDzYtSAMpOWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya7d25SLeu-x",
        "outputId": "e503fd79-95e6-4791-ba12-8d659aae1a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Preprocess Data"
      ],
      "metadata": {
        "id": "PkzogCh2pXTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "j-L683c3qAA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"https://raw.githubusercontent.com/yeesem/NLP_Dataset/main/en_US.twitter.txt\", \"r\") as f:\n",
        "#     data = f.read()\n",
        "\n",
        "# URL of the raw file\n",
        "# Extract the text from the\n",
        "url = \"https://raw.githubusercontent.com/yeesem/NLP_Dataset/main/en_US.twitter.txt\"\n",
        "\n",
        "# Send a GET request to fetch the raw file content\n",
        "response = requests.get(url)\n",
        "\n",
        "data = response.text\n",
        "\n",
        "print(\"Data type:\", type(data))\n",
        "print(\"Number of letters:\", len(data))\n",
        "print(\"First 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[0:300])\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"Last 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[-300:])\n",
        "print(\"-------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "ZqJ4XxWEgqav",
        "outputId": "db9119bd-b561-45c9-c6af-ae807838f45d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data type: <class 'str'>\n",
            "Number of letters: 3383438\n",
            "First 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\r\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\r\\nthey've decided its more fun if I don't.\\r\\nSo Tired D; Played Lazer Tag & Ran\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "Last 300 letters of the data\n",
            "-------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"had one a few weeks back....hopefully we will be back soon! wish you the best yo\\r\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\r\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\r\\nCoffee after 5 was a TERRIBLE idea.\\r\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-process the Data"
      ],
      "metadata": {
        "id": "AojJUpMoqDmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the sentences"
      ],
      "metadata": {
        "id": "U4pqaNEnqKb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the sentences\n",
        "def split_to_sentences(data):\n",
        "\n",
        "  sentences = data.split('\\n')\n",
        "\n",
        "  # - Remove leading and trailing spaces from each sentence\n",
        "  # - Drop sentences if they are empty strings.\n",
        "  sentences = [s.strip() for s in sentences]\n",
        "  sentences = [s for s in sentences if len(s) > 0]\n",
        "\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "w4RfupYIpecp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize sentences"
      ],
      "metadata": {
        "id": "jNgOWw5lqOQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentences(sentences):\n",
        "\n",
        "  # Initialize the list of lists of tokenzed sentences\n",
        "  tokenized_sentences = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "\n",
        "    # Convert to lowercase letters\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # Convert into a list of word\n",
        "    tokenized = nltk.tokenize.word_tokenize(sentence)\n",
        "\n",
        "    # Append the list of words to the list of lists\n",
        "    tokenized_sentences.append(tokenized)\n",
        "\n",
        "  return tokenized_sentences"
      ],
      "metadata": {
        "id": "C_I2yYelp9Df"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}