{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnj1SwYF/XhsPwjaLJ6NsK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUol6Dp3ycNZ",
        "outputId": "21ea42ac-e81d-4e4d-bdd3-5d1d8e95ccb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m358.4/431.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc7H2chEwOiY",
        "outputId": "082c77c8-aa09-457d-c9c0-139104284d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "# Use to handle punctuation\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "LCB_dH1YwzrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning and tokenization"
      ],
      "metadata": {
        "id": "AoKFH4HRw8YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define corpus\n",
        "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!\"'"
      ],
      "metadata": {
        "id": "1f--63KNwxie"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print original corpus\n",
        "print(f\"Corpus : {corpus}\")\n",
        "\n",
        "# Do the substitution\n",
        "data = re.sub(r'[,!?;-]', '.', corpus)\n",
        "\n",
        "# Print cleaned corpus\n",
        "print(f\"After cleaning punctuation : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctcw3Gx_xEVH",
        "outputId": "ff8780d4-e69d-47ea-f608-6e6799aeea19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus : Who ❤️ \"word embeddings\" in 2020? I do!!!\"\n",
            "After cleaning punctuation : Who ❤️ \"word embeddings\" in 2020. I do...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the cleaned corpus\n",
        "print(f\"Initial string : {data}\")\n",
        "\n",
        "# Tokenize the cleaned corpus\n",
        "data = nltk.word_tokenize(data)\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f\"After tokenization : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxJ_FwxrxZHd",
        "outputId": "676c904a-042e-4fe2-d626-ea4f9bb238f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial string : Who ❤️ \"word embeddings\" in 2020. I do...\"\n",
            "After tokenization : ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '...', \"''\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokenized version of the corpus\n",
        "print(f\"Initial list of tokens : {data}\")\n",
        "\n",
        "# Filter tokenized corpus using list comprehension\n",
        "data  = [\n",
        "    ch.lower() for ch in data\n",
        "    if ch.isalpha()\n",
        "    or ch == '.'\n",
        "    or bool(emoji.emoji_list(ch))\n",
        "]\n",
        "\n",
        "# Print the tokenized and filtered version of the corpus\n",
        "print(f\"After cleaning : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy9-PFCsxyid",
        "outputId": "83dc7564-8c84-4102-aed4-3b62fdb624b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial list of tokens : ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '...', \"''\"]\n",
            "After cleaning : ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "  data = nltk.word_tokenize(data)\n",
        "  data = [\n",
        "      ch.lower() for ch in data\n",
        "      if ch.isalpha()\n",
        "      or ch == '.'\n",
        "      or bool(emoji.emoji_list(ch))\n",
        "  ]\n",
        "  return data"
      ],
      "metadata": {
        "id": "nl3EIhhFz6wG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new corpus\n",
        "corpus = \"I am happy because I am learning\"\n",
        "\n",
        "# Print new corpus\n",
        "print(f\"Corpus : {corpus}\")\n",
        "\n",
        "# Save tokenized version of corpus into 'words' variable\n",
        "words = tokenize(corpus)\n",
        "\n",
        "# Print the tokenzied version of the corpus\n",
        "print(f\"Words (tokens) : {words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVk4goGM0xwo",
        "outputId": "cea6844b-0e38-443d-f213-1e6d4cf85931"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus : I am happy because I am learning\n",
            "Words (tokens) : ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this with any sentence\n",
        "tokenize(\"My name is John. How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gadHGdoJ1TH-",
        "outputId": "409f3466-78ce-4d4f-c300-2ae20fae25ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['my', 'name', 'is', 'john', '.', 'how', 'are', 'you', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sliding window of words"
      ],
      "metadata": {
        "id": "Z6EQHpNl1aEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 'get_windows' function\n",
        "def get_windows(words, C):\n",
        "  i = C\n",
        "  while i < len(words) - C:\n",
        "    center_word = words[i]\n",
        "    context_words = words[(i - C) : i] + words[(i + 1) : (i + C + 1)]\n",
        "    yield context_words, center_word\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "FivvHlZy1bhi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
        "  print(f\"{x}\\t{y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr5TiLIX2cgJ",
        "outputId": "04b1b37c-a108-45aa-bb83-2ec650f058b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'because', 'i']\thappy\n",
            "['am', 'happy', 'i', 'am']\tbecause\n",
            "['happy', 'because', 'am', 'learning']\ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print 'context_words' and 'center_word' for any sentence with a 'context half-size' of 1\n",
        "for x, y in get_windows(tokenize(\"My name is John. How are you?\"), 1):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkZZgKSR2p6T",
        "outputId": "681ef952-c6f4-4b68-d38d-df455e16c01f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'is']\tname\n",
            "['name', 'john']\tis\n",
            "['is', '.']\tjohn\n",
            "['john', 'how']\t.\n",
            "['.', 'are']\thow\n",
            "['how', 'you']\tare\n",
            "['are', '.']\tyou\n"
          ]
        }
      ]
    }
  ]
}