{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa/aj8fO2M0HSgLiC4AMus"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUol6Dp3ycNZ",
        "outputId": "21ea42ac-e81d-4e4d-bdd3-5d1d8e95ccb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m358.4/431.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc7H2chEwOiY",
        "outputId": "082c77c8-aa09-457d-c9c0-139104284d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "# Use to handle punctuation\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "LCB_dH1YwzrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning and tokenization"
      ],
      "metadata": {
        "id": "AoKFH4HRw8YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define corpus\n",
        "corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!\"'"
      ],
      "metadata": {
        "id": "1f--63KNwxie"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print original corpus\n",
        "print(f\"Corpus : {corpus}\")\n",
        "\n",
        "# Do the substitution\n",
        "data = re.sub(r'[,!?;-]', '.', corpus)\n",
        "\n",
        "# Print cleaned corpus\n",
        "print(f\"After cleaning punctuation : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctcw3Gx_xEVH",
        "outputId": "ff8780d4-e69d-47ea-f608-6e6799aeea19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus : Who ❤️ \"word embeddings\" in 2020? I do!!!\"\n",
            "After cleaning punctuation : Who ❤️ \"word embeddings\" in 2020. I do...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the cleaned corpus\n",
        "print(f\"Initial string : {data}\")\n",
        "\n",
        "# Tokenize the cleaned corpus\n",
        "data = nltk.word_tokenize(data)\n",
        "\n",
        "# Print the tokenized version of the corpus\n",
        "print(f\"After tokenization : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxJ_FwxrxZHd",
        "outputId": "676c904a-042e-4fe2-d626-ea4f9bb238f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial string : Who ❤️ \"word embeddings\" in 2020. I do...\"\n",
            "After tokenization : ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '...', \"''\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokenized version of the corpus\n",
        "print(f\"Initial list of tokens : {data}\")\n",
        "\n",
        "# Filter tokenized corpus using list comprehension\n",
        "data  = [\n",
        "    ch.lower() for ch in data\n",
        "    if ch.isalpha()\n",
        "    or ch == '.'\n",
        "    or bool(emoji.emoji_list(ch))\n",
        "]\n",
        "\n",
        "# Print the tokenized and filtered version of the corpus\n",
        "print(f\"After cleaning : {data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy9-PFCsxyid",
        "outputId": "83dc7564-8c84-4102-aed4-3b62fdb624b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial list of tokens : ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '...', \"''\"]\n",
            "After cleaning : ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "  data = nltk.word_tokenize(data)\n",
        "  data = [\n",
        "      ch.lower() for ch in data\n",
        "      if ch.isalpha()\n",
        "      or ch == '.'\n",
        "      or bool(emoji.emoji_list(ch))\n",
        "  ]\n",
        "  return data"
      ],
      "metadata": {
        "id": "nl3EIhhFz6wG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define new corpus\n",
        "corpus = \"I am happy because I am learning\"\n",
        "\n",
        "# Print new corpus\n",
        "print(f\"Corpus : {corpus}\")\n",
        "\n",
        "# Save tokenized version of corpus into 'words' variable\n",
        "words = tokenize(corpus)\n",
        "\n",
        "# Print the tokenzied version of the corpus"
      ],
      "metadata": {
        "id": "aVk4goGM0xwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}