{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwQ5nI4BaZG/TpQH/ZTXoA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReAI98cZemDM",
        "outputId": "506b2b18-8b2f-4687-fb9f-4120c98d88d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new vocabulary containing 3 most frequent words: ['happy', 'because', 'learning']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Build the vocabulary from M most frequent words\n",
        "# Use Counter object from the collections library to find M most common words\n",
        "from collections import Counter\n",
        "\n",
        "# The target size of the vocabulary\n",
        "M = 3\n",
        "\n",
        "# Counter could be used to build this dictionary from the source corpus\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n",
        "\n",
        "# most_common gets the word >= M(3 )\n",
        "vocabulary = Counter(word_counts).most_common(M)\n",
        "\n",
        "# Remove the frequencies and leave just the words\n",
        "vocabulary = [w[0] for w in vocabulary]\n",
        "\n",
        "print(f\"The new vocabulary containing {M} most frequent words: {vocabulary}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['am','i','learning']\n",
        "output_sentence = []\n",
        "print(f\"Input sentence : {sentence}\")\n",
        "\n",
        "for w in sentence:\n",
        "  # test if word w is in vocabulary\n",
        "  if w in vocabulary:\n",
        "    output_sentence.append(w)\n",
        "  else:\n",
        "    output_sentence.append(\"<UNK>\")\n",
        "\n",
        "print(f\"Output sentence : {output_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1NNiMZ4gPi0",
        "outputId": "95dcb0d4-a1ee-4c41-b6db-fcd9109bbec5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence : ['am', 'i', 'learning']\n",
            "Output sentence : ['<UNK>', '<UNK>', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through all word counts and print words with given frequency f\n",
        "f = 3\n",
        "\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
        "\n",
        "for word,freq in word_counts.items():\n",
        "  if freq == f:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmnmF149gw8p",
        "outputId": "4fd86453-c14f-4bbb-9f76-36b59ab3f8ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "because\n",
            "learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# many <unk> low perplexity\n",
        "training_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\n",
        "training_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n",
        "\n",
        "test_set = ['i', 'am', 'learning']\n",
        "test_set_unk = ['i', 'am', '<UNK>']\n",
        "\n",
        "M = len(test_set)\n",
        "probability = 1\n",
        "probability_unk = 1\n",
        "\n",
        "# pre-calculated probabilities\n",
        "bigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\n",
        "bigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n",
        "\n",
        "# Got through the test set and calculate its bigram probability\n",
        "for i in range(len(test_set) - 2 + 1):\n",
        "  bigram = tuple(test_set[i : i + 2])\n",
        "  probability = probability * bigram_probabilities[bigram]\n",
        "\n",
        "  bigram_unk = tuple(test_set_unk[i : i + 2])\n",
        "  probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n",
        "\n",
        "# Calculate perplexity for both original test set and test set with <UNK>\n",
        "perplexity = probability ** (-1/M)\n",
        "perplexity_unk = probability_unk ** (-1/M)\n",
        "\n",
        "print(f\"Perplexity for the training set : \", {perplexity})\n",
        "print(f\"Perplexity for the training set with <UNK>: \", {perplexity_unk})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWSKI0Uq3-T2",
        "outputId": "b2545943-47bd-4e64-e1df-90ac930f5522"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for the training set :  {1.2599210498948732}\n",
            "Perplexity for the training set with <UNK>:  {1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing"
      ],
      "metadata": {
        "id": "O91DZ_p95nLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_K_smoothing_probability(k,vocabulary_size,n_gram_count,n_gram_prefix_count):\n",
        "  numerator = n_gram_count + k\n",
        "  denominator = n_gram_prefix_count + k * vocabulary_size\n",
        "  return numerator / denominator\n",
        "\n",
        "trigram_probabilities =  {('i', 'am', 'happy') : 2}\n",
        "bigram_probabilities = {( 'i', 'am') : 10}\n",
        "vocabulary_size = 5\n",
        "k = 1\n",
        "\n",
        "probability_known_trigram = add_K_smoothing_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')],\n",
        "                                                        bigram_probabilities[( 'i', 'am')])\n",
        "probability_unknown_trigram = add_K_smoothing_probability(k, vocabulary_size, 0, 0)\n",
        "\n",
        "print(f\"probability_known_trigram   : {probability_known_trigram}\")\n",
        "print(f\"probability_unknown_trigram : {probability_unknown_trigram}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7u0dHYK5paA",
        "outputId": "fe6f5503-cbec-49c9-c305-4bccf231914f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability_known_trigram   : 0.2\n",
            "probability_unknown_trigram : 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Back-off"
      ],
      "metadata": {
        "id": "bAdwXII-HOfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Back-off is a model generalization method that leverages information from lower order n-grams\n",
        "# in case information about the high order n-grams is missing. For example, if the probability\n",
        "# of an trigram is missing, use bigram information and so on.\n",
        "# pre-calculated probabilities of all types of n-grams\n",
        "trigram_probabilities = {('i', 'am', 'happy'): 0}\n",
        "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
        "unigram_probabilities = {'happy': 0.4}\n",
        "\n",
        "# This is the input trigram we need to estimate\n",
        "trigram = ('are', 'you', 'happy')\n",
        "\n",
        "# Find the last bigram and unigram of the input\n",
        "bigram = trigram[1:3]\n",
        "unigram = trigram[2]\n",
        "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
        "\n",
        "# 0.4 is used as an example, experimentally found for web-scale corpuses when using the \"stupid\" back-off\n",
        "lambda_factor = 0.4\n",
        "probability_hat_trigram = 0\n",
        "\n",
        "if trigram not in trigram_probabilities or trigram_probabilities[trigram] == 0:\n",
        "  print(f\"Probability for trigram {trigram} not found\")\n",
        "\n",
        "  if bigram not in bigram_probabilities or bigram_probabilities[bigram] == 0:\n",
        "    print(f\"Probability for bigram {bigram} not found\")\n",
        "\n",
        "    if unigram in unigram_probabilities:\n",
        "      print(f\"Probability for unigram {unigram} found\\n\")\n",
        "      probability_hat_trigram = lambda_factor * lambda_factor * unigram_probabilities[unigram]\n",
        "\n",
        "    else:\n",
        "      probability_hat_trigram = 0\n",
        "\n",
        "  else:\n",
        "    probability_hat_trigram = trigram_probabilities[trigram]\n",
        "\n",
        "  print(f\"Probability for trigram {trigram} estimated as {probability_hat_trigram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcCSopkoHQSs",
        "outputId": "8b0841d0-1344-4648-eaf1-703125278c51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "besides the trigram ('are', 'you', 'happy') we also use bigram ('you', 'happy') and unigram (happy)\n",
            "\n",
            "Probability for trigram ('are', 'you', 'happy') not found\n",
            "Probability for bigram ('you', 'happy') not found\n",
            "Probability for unigram happy found\n",
            "\n",
            "Probability for trigram ('are', 'you', 'happy') estimated as 0.06400000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpolation"
      ],
      "metadata": {
        "id": "28nFSq3-Jfa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-calculated probabilities of all types of n-grams\n",
        "trigram_probabilities = {('i', 'am', 'happy'): 0.15}\n",
        "bigram_probabilities = {( 'am', 'happy'): 0.3}\n",
        "unigram_probabilities = {'happy': 0.4}\n",
        "\n",
        "# The weights come from optimazation on a validation set\n",
        "# The summation of all lambdas must be equal to one\n",
        "lambda_1 = 0.8\n",
        "lambda_2 = 0.15\n",
        "lambda_3 = 0.05\n",
        "\n",
        "# this is the input trigram we need to estimate\n",
        "trigram = ('i', 'am', 'happy')\n",
        "\n",
        "# Find the last bigram and unigram of the input\n",
        "bigram = trigram[1:3]\n",
        "unigram = trigram[2]\n",
        "print(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n",
        "\n",
        "# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\n",
        "probability_hat_trigram = lambda_1 * trigram_probabilities[trigram]\n",
        "+ lambda_2 * bigram_probabilities[bigram]\n",
        "+ lambda_3 * unigram_probabilities[unigram]\n",
        "\n",
        "print(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB0NGtoWJhJq",
        "outputId": "efa86e86-a56f-4dca-e0c6-02449bde575e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "besides the trigram ('i', 'am', 'happy') we also use bigram ('am', 'happy') and unigram (happy)\n",
            "\n",
            "estimated probability of the input trigram ('i', 'am', 'happy') is 0.12\n"
          ]
        }
      ]
    }
  ]
}