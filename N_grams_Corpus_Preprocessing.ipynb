{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMdRR+PXV9uk1nP04hBdAc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "YZG8MGI1xxNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj-fr0J9tkU5",
        "outputId": "67fb6706-aca1-418e-de06-8a2fc183b94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercase"
      ],
      "metadata": {
        "id": "wF6reodPxzlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the corpus to lowercase\n",
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "\n",
        "# Note that word 'learning' will now be the same regardless of its position in\n",
        "# the sentence\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yn9PLfht5Mq",
        "outputId": "f967d70d-66ac-4c5e-ffac-33708e88625f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Special Characters"
      ],
      "metadata": {
        "id": "M65TPOdyx3in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters\n",
        "corpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\n",
        "\n",
        "# a-zA-Z: Matches any lowercase or uppercase letter.\n",
        "# 0-9: Matches any digit.\n",
        "# .!? : Matches a period, exclamation mark, question mark, or space.\n",
        "corpus = re.sub(r'[^a-zA-z0-9.?! ]+',\"\",corpus)\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpOOFU8eulpG",
        "outputId": "d68ee101-3f47-4c67-c061-89d38f81c27d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning makes me happy. i am happy because i am learning! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Spliting"
      ],
      "metadata": {
        "id": "wAz-v3vrx6iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split text by a delimiter to array\n",
        "input_date = \"Sat May  9 07:33:35 CEST 2020\"\n",
        "\n",
        "# Get the date parts in array\n",
        "date_parts = input_date.split(\" \")\n",
        "print(f\"date parts = {date_parts}\")\n",
        "\n",
        "# Get the time parts in array\n",
        "time_parts = date_parts[4].split(\":\")\n",
        "print(f\"time parts = {time_parts}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwXTSpoewvh_",
        "outputId": "1e036cef-80eb-41a0-973f-2e9116ae4c60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date parts = ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
            "time parts = ['07', '33', '35']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenizing"
      ],
      "metadata": {
        "id": "1Rn1g7jJx99s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence tokenizing\n",
        "# Tokenize the sentence into an array of words\n",
        "sentence = 'i am happy because i am learning'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f\"{sentence} -> {tokenized_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xyViYjGxudJ",
        "outputId": "9c95e1c4-2d34-4338-82dc-630259462782"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find length of each word in the tokenized sentence\n",
        "sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "# Create a list with the word lengths using a list comprehension\n",
        "word_lengths = [(word,len(word)) for word in sentence]\n",
        "print(f\"Lengths of the words : \\n{word_lengths}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9MEjDeFySD9",
        "outputId": "8503a3ab-da2d-4a40-b195-c05c563d3b5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lengths of the words : \n",
            "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-grams"
      ],
      "metadata": {
        "id": "DwVZ08GYysI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_trigam(tokenized_sentence):\n",
        "  for i in range(len(tokenized_sentence) - 3 + 1):\n",
        "    # The sliding window starts at position i and contains 3 words\n",
        "    trigram = tokenized_sentence[i : i + 3]\n",
        "    print(trigram)\n",
        "\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "\n",
        "print(f\"List all trigrams of sentence : {tokenized_sentence}\\n\")\n",
        "sentence_to_trigam(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yeteWZVyvdL",
        "outputId": "57270d59-d8b8-4c57-914f-2171cc4236dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all trigrams of sentence : ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
            "\n",
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prefix of an n-gram"
      ],
      "metadata": {
        "id": "vLpH8kxS0lI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get trigam prefix from a 4-gram\n",
        "fourgram = ['i', 'am', 'happy', 'because']\n",
        " # Get the elements from 0, included, up to the last element, not included.\n",
        "trigram = fourgram[0:-1]\n",
        "print(trigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14j0k2JU0nvX",
        "outputId": "487beade-12b3-42b3-a818-65215aac8feb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start and end of sentence word  <𝑠> and  <𝑒>\n",
        "\n"
      ],
      "metadata": {
        "id": "SROyhF8O1DTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when working with trigrams, you need to prepend 2 <s> and append one </s>\n",
        "n = 3\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "tokenized_sentence = [\"<s>\"] * (n-1) + tokenized_sentence + [\"<e>\"]\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtndtvLX09co",
        "outputId": "2b4b334b-e374-406f-c6c9-f4fcb69d5228"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '<e>']\n"
          ]
        }
      ]
    }
  ]
}